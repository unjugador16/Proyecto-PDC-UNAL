Todos lo siguientes puntos se realizaron tomando en cuanta que el texto pasó por una limpieza, en donde se eliminaron acentos no pertenecientes al abecedario (é ü) y signos (¿/), teniendo en cuenta el idioma.
7.	Estrategia #1: con un diccionario de todos las letras presentes en el texto (con y sin acentos) y sus respectivas frecuencias, se revisa (con un conjunto con todos los caracteres con signos diacriticos por cada uno de los idiomas) la presencia de ciertos caracteres clave que permitan identificar el idioma mediante el uso de condicionales encadenados.
Retos al implementar la estrategia y como se abordaron: debido a que no se puede garantizar la presencia de los caracteres clave para la identificacion del idioma en el texto, no se puede garantizar que el programa identifique de manera adecuada el idioma. Por ende, otra manera de implementarlo es encadenar varios condicionales que logren, segun los diacriticos que aparezcan, encasillar el texto en dos idiomas distintos con signos diacriticos compartidos que no esten presentes en el resto de idiomas. De esa manera, si estos caracteres compartidos estan presentes en el texto, se usan los caracteres presentes en un solo idima para identificar cual de los dos restantes es, y en caso de que los caracteres compartidos no esten presentes, se compara con los conjunto de diacriticos de los otros idiomas. De esta manera, si se encuentra que los diacriticos pertenecen a cierto idioma, se identifica cual es, y si no se encuentran diacriticos, se puede inferir que es ingles.
Análisis de resultados: si bien la funcion cumple con su cometido la mayoria de veces, algunas veces no es capaz de detectar el idioma, y retorna idioma ingles. No es completamente funcional.

Estrategia #2: con un diccionario de todos las letras presentes en el texto (con y sin acentos) y sus respectivas frecuencias, se revisa (con un conjunto con todos los caracteres con signos diacriticos por cada uno de los idiomas) la presencia de ciertos caracteres clave que permitan identificar el idioma mediante el uso de for para explorar elemento por elemento del diccionario y buscar si esta en alguno de los conjuntos correspondientes a los idiomas. De ser asi, se aumenta un contador de ese idioma al que pertenece el caracter por la cantidad de veces que aparece tal caracter en el texto. El mayor de los contadores sera el correspondiente al idioma y en caso de ser 0 o menor al 0.1% del texto, el idioma seria ingles por la falta de diacriticos en el texto.
Retos al implementar la estrategia y como se abordaron: a pesar de que no se encontraron contratiempos ni retos al implementar la estrategia, se teme que por la cantidad de procesos de busqueda dentro del for pueda ralentizar el proceso.
Análisis de resultados: la funcion es capaz de identificar el idioma adecuadamente en todos los casos de prueba.

9.	Estrategia #1: usar la libreria spacy para procesar el texto, y dependiendo del idioma del texto, se usa un modelo de lenguaje diferente para el idioma del texto. Crear un diccionario con todas las palabras que cominecen por una mayuscula, que tengan una longitud estrictamente mayor a dos, que no sean precedidas por un punto, signo de interrogacion o signo de exclamacion, que no tengan una copia de la palabra en minuscula en el texto y que sean reconocidas como nombres propios por spacy. Luego, separar las palabras/entidades a un diccionario de frecuencias de solo nombres con el label de entidad "PER". Esta funcion se hace en conjunto con el 10.
Retos al implementar la estrategia y como se abordaron: A pesar de que encontrar palabras que empiecen por una mayuscula no es dificil, distinguirlas de nombres es mas complicado. Por ello se metieron varias condiciones para evitar que se metieran palabras no correspondientes a nombre propios. A pesar de eso, debido a la imprecision del modelo de spacy para el lenguaje español, igual se llegaron a filtrar bastantes palabras. Por ello, se decidio eliminar todos los nombres con una sola aparicion. Ademas, se obvia la presencia de una mayuscula al principio deel nombre, pero en caso de apodos y pronombres, estos no son contados, a pesar de que hacen referencia a algun personaje. 
Análisis de resultados: A pesar de todos los cambios implementados, muchas palabras "basura" se siguen filtrando en el resultado final.

10.	Estrategia #1: usar la libreria spacy para procesar el texto, y dependiendo del idioma del texto, se usa un modelo de lenguaje diferente para el idioma del texto. Crear un diccionario con todas las palabras que cominecen por una mayuscula, que tengan una longitud estrictamente mayor a dos, que no sean precedidas por un punto, signo de interrogacion o signo de exclamacion, que no tengan una copia de la palabra en minuscula en el texto y que sean reconocidas como nombres propios por spacy. Luego, separar las palabras/entidades a un conjunto de solo lugares con el label de entidad "LOC". Esta funcion se hace en conjunto con el 9.
Retos al implementar la estrategia y como se abordaron: A pesar de que encontrar palabras que empiecen por una mayuscula no es dificil, distinguirlas de nombres es mas complicado. Por ello se metieron varias condiciones para evitar que se metieran palabras no correspondientes a nombre propios de ciudades. A pesar de eso, debido a la imprecision del modelo de spacy para el lenguaje español, igual se llegaron a filtrar bastantes palabras. Por ello, se decidio eliminar todos los lugares con una sola aparicion. Ademas, se obvia la presencia de una mayuscula al principio del nombre del lugar, pero en caso de pronombres, estos no son contados, a pesar de que hacen referencia a algun lugar. 
Análisis de resultados: A pesar de todos los cambios implementados, muchas palabras "basura" se siguen filtrando en el resultado final.


11.	Estrategia #1: Se usa como base el diccionario de frecuencia de nombres, y se incluyen en una lista solo aquellos que aparecen al menos cada 450 palabras aproximadamente. 
Retos al implementar la estrategia y como se abordaron: encontrar un numero adecuado para hacer de frontera entre personaje principal y secundario fue bastante decisivo. En varias obras en la que se aplico la funcion los numeros para incluir a sus personajes principales y excluir a los secundarios eran bastante diferentes. Despues de bastante prueba y error, se opto por definir la frecuencia por cada 450 palabras.
Análisis de resultados: No es completamente preciso debido a la imprecision del diccionario de presonajes.

12.	Estrategia #1: g
Retos al implementar la estrategia y como se abordaron: g
Análisis de resultados: n
